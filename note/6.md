# 实时推荐

> 此模块的前置条件：
>
> 1. 前几个模块运行成功
> 2. 虚拟机安装完成Redis、Kafka和Flume并且成功启动
> 3. 成功构建日志文件——Flume——Kafka——Spark Streaming的数据流

1. 新建子模块OnlineRecommender

2. 添加依赖

   ```xml
   <?xml version="1.0" encoding="UTF-8"?>
   <project xmlns="http://maven.apache.org/POM/4.0.0"
            xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
            xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
       <parent>
           <artifactId>book-recommender</artifactId>
           <groupId>com.wan</groupId>
           <version>1.0-SNAPSHOT</version>
       </parent>
       <modelVersion>4.0.0</modelVersion>
   
       <artifactId>OnlineRecommender</artifactId>
   
       <dependencies>
           <!-- Spark的依赖引入 -->
           <dependency>
               <groupId>org.apache.spark</groupId>
               <artifactId>spark-core_2.11</artifactId>
           </dependency>
           <dependency>
               <groupId>org.apache.spark</groupId>
               <artifactId>spark-sql_2.11</artifactId>
           </dependency>
           <dependency>
               <groupId>org.apache.spark</groupId>
               <artifactId>spark-streaming_2.11</artifactId>
           </dependency>
           <!-- 引入Scala -->
           <dependency>
               <groupId>org.scala-lang</groupId>
               <artifactId>scala-library</artifactId>
           </dependency>
   
           <!-- 加入MongoDB的驱动 -->
           <!-- 用于代码方式连接MongoDB -->
           <dependency>
               <groupId>org.mongodb</groupId>
               <artifactId>casbah-core_2.11</artifactId>
               <version>${casbah.version}</version>
           </dependency>
           <!-- 用于Spark和MongoDB的对接 -->
           <dependency>
               <groupId>org.mongodb.spark</groupId>
               <artifactId>mongo-spark-connector_2.11</artifactId>
               <version>${mongodb-spark.version}</version>
           </dependency>
   
           <!-- redis -->
           <dependency>
               <groupId>redis.clients</groupId>
               <artifactId>jedis</artifactId>
               <version>2.9.0</version>
           </dependency>
   
           <!-- kafka -->
           <dependency>
               <groupId>org.apache.kafka</groupId>
               <artifactId>kafka-clients</artifactId>
               <version>0.10.2.1</version>
           </dependency>
           <dependency>
               <groupId>org.apache.spark</groupId>
               <artifactId>spark-streaming-kafka-0-10_2.11</artifactId>
               <version>${spark.version}</version>
           </dependency>
   
       </dependencies>
   
   </project>
   ```

3. 配置日志文件

4. 编码`OnlineRecommender.scala`

   ```scala
   package com.wan.online
   
   import com.mongodb.casbah.commons.MongoDBObject
   import com.mongodb.casbah.{MongoClient, MongoClientURI}
   import org.apache.kafka.common.serialization.StringDeserializer
   import org.apache.spark.SparkConf
   import org.apache.spark.sql.SparkSession
   import org.apache.spark.streaming.kafka010.{ConsumerStrategies, KafkaUtils, LocationStrategies}
   import org.apache.spark.streaming.{Seconds, StreamingContext}
   import redis.clients.jedis.Jedis
   
   /**
    * @author wanfeng
    * @date 2021/3/13 18:39
    */
   
   //定义一个连接助手对象，建立Redis到MongoDB的连接
   object ConnHelper extends Serializable {
     lazy val jedis = new Jedis("192.168.2.88")
     lazy val mongoClient = MongoClient(MongoClientURI("mongodb://192.168.2.88:27017/recommender"))
   }
   
   case class MongoConfig(uri: String, db: String)
   
   case class Recommenderation(bookId: Int, score: Double)
   
   case class UserRecs(userId: Int, recs: Seq[Recommenderation])
   
   case class BookRecs(bookId: Int, recs: Seq[Recommenderation])
   
   object OnlineRecommender {
   
     val MONGODB_RATING_COLLECTION = "Rating"
     // 定义表名
     val STREAM_RECS = "StreamRecs"
     val BOOK_RECS = "BookRecs"
   
     val MAX_USER_RATINGS_NUM = 20
     val MAX_SIM_BOOKS_NUM = 20
   
     def main(args: Array[String]): Unit = {
   
       val config = Map(
         "spark.cores" -> "local[*]",
         "mongo.uri" -> "mongodb://192.168.2.88:27017/recommender",
         "mongo.db" -> "recommender",
         "kafka.topic" -> "recommender"
       )
   
       val sparkConf = new SparkConf().setMaster(config("spark.cores")).setAppName("OnlineRecommender")
       val spark = SparkSession.builder().config(sparkConf).getOrCreate()
       val sc = spark.sparkContext
       val ssc = new StreamingContext(sc, Seconds(2))
   
       import spark.implicits._
       implicit val mongoConfig = MongoConfig(config("mongo.uri"), config("mongo.db"))
   
   
       //加载数据，相似度矩阵，广播出去
       val simBooksMatrix = spark.read
         .option("uri", mongoConfig.uri)
         .option("collection", BOOK_RECS)
         .format("com.mongodb.spark.sql")
         .load()
         .as[BookRecs]
         .rdd
         .map { item =>
           (
             item.bookId, item.recs.map(x => (x.bookId, x.score)).toMap
           )
         }
         .collectAsMap()
   
       //定义广播变量
       val simBooksMatrixBC = sc.broadcast(simBooksMatrix)
   
       //创建kafka的连接
       val kafkaPara = Map(
         "bootstrap.servers" -> "192.168.2.88:9092",
         "key.deserializer" -> classOf[StringDeserializer],
         "value.deserializer" -> classOf[StringDeserializer],
         "group.id" -> "recommender",
         "auto.offset.reset" -> "latest"
       )
       //创建一个DStream
       val kafkaStream = KafkaUtils.createDirectStream[String, String](
         ssc,
         LocationStrategies.PreferConsistent,
         ConsumerStrategies.Subscribe[String, String](Array(config("kafka.topic")), kafkaPara)
       )
   
       //对kafkaStream进行处理，产生评分流，
       val ratingStream = kafkaStream.map {
         msg =>
           val attr = msg.value().split("\\|")
           (
             attr(0).toInt,
             attr(1).toInt,
             attr(2).toDouble
           )
       }
   
       //核心算法，定义评分流的处理流程
       ratingStream.foreachRDD {
         rdds =>
           rdds.map {
             case (userId, bookId, score) =>
               println("rating data coming...")
               println(userId + "----")
               println(bookId + "----")
               println(score + "----")
   
               //TODO：核心算法流程
               //1.从redis中取出当前用户的最近评分，保存成一个数组Array[(bookId,score)]
   
               val userRecentlyRatings = getUserRecentlyRating(MAX_USER_RATINGS_NUM, userId, ConnHelper.jedis)
               //2.从相似度矩阵中获取当前商品最相似的商品列表，作为备选列表
               val candidateBooks = getTopSimBooks(MAX_SIM_BOOKS_NUM, bookId, userId, simBooksMatrixBC.value)
               //3.计算每个备选商品的推荐优先级，得到当前用户的实时推荐列表
               val streamRecs = computeBookScore(candidateBooks, userRecentlyRatings, simBooksMatrixBC.value)
   
               //4.把推荐列表保存到mongodb
               saveDataToMongoDB(userId, streamRecs)
           }.count()
       }
       //启动streaming
       ssc.start()
       println("streaming started!")
       ssc.awaitTermination()
     }
   
     import scala.collection.JavaConversions._
   
     def getUserRecentlyRating(num: Int, userId: Int, jedis: Jedis): Array[(Int, Double)] = {
       jedis.lrange("userId:" + userId.toString, 0, num)
         .map {
           item =>
             val attr = item.split("\\:")
             (
               attr(0).trim.toInt,
               attr(1).trim.toDouble
             )
         }
         .toArray
   
     }
   
     def getTopSimBooks(num: Int,
                        bookId: Int,
                        userId: Int,
                        simBooks: scala.collection.Map[Int, scala.collection.immutable.Map[Int, Double]])
                       (implicit mongoConfig: MongoConfig): Array[Int] = {
       val allSimBooks = simBooks(bookId).toArray
   
       val ratingCollection = ConnHelper.mongoClient(mongoConfig.db)(MONGODB_RATING_COLLECTION)
       val ratingExist = ratingCollection.find(MongoDBObject("userId" -> userId))
         .toArray
         .map {
           item =>
             item.get("bookId").toString.toInt
         }
       allSimBooks.filter(x => !ratingExist.contains(x._1))
         .sortWith(_._2 > _._2)
         .take(num)
         .map(x => x._1)
     }
   
     def computeBookScore(candidateBooks: Array[Int],
                          userRecentlyRatings: Array[(Int, Double)],
                          simBooks: scala.collection.Map[Int, scala.collection.immutable.Map[Int, Double]])
     : Array[(Int, Double)] = {
       val scores = scala.collection.mutable.ArrayBuffer[(Int, Double)]()
   
       val increMap = scala.collection.mutable.HashMap[Int, Int]()
       val decreMap = scala.collection.mutable.HashMap[Int, Int]()
   
       for (candidateBook <- candidateBooks; userRecentlyRating <- userRecentlyRatings) {
         val simScore = getBooksSimScore(candidateBook, userRecentlyRating._1, simBooks)
         if (simScore > 0.4) {
           scores += ((candidateBook, simScore * userRecentlyRating._2))
           if (userRecentlyRating._2 > 6) {
             increMap(candidateBook) = increMap.getOrDefault(candidateBook, 0) + 1
           } else {
             decreMap(candidateBook) = decreMap.getOrDefault(candidateBook, 0) + 1
           }
         }
       }
   
       scores.groupBy(_._1).map {
         case (bookId, scoreList) =>
           (bookId, scoreList.map(_._2).sum / scoreList.length + log(increMap.getOrDefault(bookId, 1)) - log(decreMap.getOrDefault(bookId, 1)))
       }
         .toArray
         .sortWith(_._2 > _._2)
     }
   
     def getBooksSimScore(book1: Int, book2: Int, simBooks: scala.collection.Map[Int, scala.collection.immutable.Map[Int, Double]]): Double = {
       simBooks.get(book1) match {
         case Some(sims) => sims.get(book2) match {
           case Some(score) => score
           case None => 0.0
         }
         case None => 0.0
       }
     }
   
     def log(m: Int): Double = {
       val N = 10
       math.log(m) / math.log(N)
     }
   
     def saveDataToMongoDB(userId: Int, streamRecs: Array[(Int, Double)])(implicit mongoConfig: MongoConfig): Unit = {
       val streamCollection = ConnHelper.mongoClient(mongoConfig.db)(STREAM_RECS)
   
       streamCollection.findAndRemove(MongoDBObject("userId" -> userId))
       streamCollection.insert(MongoDBObject("userId" -> userId, "recs" ->
         streamRecs.map(x => MongoDBObject("bookId" -> x._1, "score" -> x._2)))
       )
     }
   }
   
   ```

5. 新建子模块KafkaStreaming

6. 添加依赖

   ```xml
   <?xml version="1.0" encoding="UTF-8"?>
   <project xmlns="http://maven.apache.org/POM/4.0.0"
            xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
            xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
       <parent>
           <artifactId>book-recommender</artifactId>
           <groupId>com.wan</groupId>
           <version>1.0-SNAPSHOT</version>
       </parent>
       <modelVersion>4.0.0</modelVersion>
   
       <artifactId>KafkaStreaming</artifactId>
   
       <dependencies>
           <dependency>
               <groupId>org.apache.kafka</groupId>
               <artifactId>kafka-streams</artifactId>
               <version>0.10.2.1</version>
           </dependency>
           <dependency>
               <groupId>org.apache.kafka</groupId>
               <artifactId>kafka-clients</artifactId>
               <version>0.10.2.1</version>
           </dependency>
       </dependencies>
   
       <build>
           <finalName>kafkastream</finalName>
           <plugins>
               <plugin>
                   <groupId>org.apache.maven.plugins</groupId>
                   <artifactId>maven-assembly-plugin</artifactId>
                   <configuration>
                       <archive>
                           <manifest>
                               <mainClass>com.wan.kafkastreaming.Application</mainClass>
                           </manifest>
                       </archive>
                       <descriptorRefs>
                           <descriptorRef>jar-with-dependencies</descriptorRef>
                       </descriptorRefs>
                   </configuration>
                   <executions>
                       <execution>
                           <id>make-assembly</id>
                           <phase>package</phase>
                           <goals>
                               <goal>single</goal>
                           </goals>
                       </execution>
                   </executions>
               </plugin>
           </plugins>
       </build>
   
   </project>
   ```

7. 配置日志文件

8. 编码`Application.java`

   ```java
   package com.wan.kafkastreaming;
   
   import org.apache.kafka.streams.KafkaStreams;
   import org.apache.kafka.streams.StreamsConfig;
   import org.apache.kafka.streams.processor.TopologyBuilder;
   
   import java.util.Properties;
   
   /**
    * @author wanfeng
    * @date 2021/3/13 18:43
    */
   public class Application {
   	public static void main(String[] args) {
   
   		String brokers = "192.168.2.88:9092"; // Kafka端口
   		String zookeepers = "192.168.2.88:2181"; // Zookeeper端口
   
   		// 定义输入和输出的topic
   		String from = "log";
   		String to = "recommender";
   
   		Properties settings = new Properties();
   		settings.put(StreamsConfig.APPLICATION_ID_CONFIG, "logFilter");
   		settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, brokers);
   		settings.put(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, zookeepers);
   		settings.put(StreamsConfig.TIMESTAMP_EXTRACTOR_CLASS_CONFIG, MyEventTimeExtractor.class);
   
   		StreamsConfig config = new StreamsConfig(settings);
   
   		TopologyBuilder builder = new TopologyBuilder();
   
   		builder
   				.addSource("SOURCE", from)
   				.addProcessor("PROCESSOR", () -> new LogProcessor(), "SOURCE")
   				.addSink("SINK", to, "PROCESSOR");
   
   		KafkaStreams streams = new KafkaStreams(builder, config);
   		streams.start();
   		System.out.println("kafka stream started!");
   	}
   }
   ```

9. 编码`LogProcessor.java`

   ```java
   package com.wan.kafkastreaming;
   
   import org.apache.kafka.streams.processor.Processor;
   import org.apache.kafka.streams.processor.ProcessorContext;
   
   /**
    * @author wanfeng
    * @date 2021/3/13 18:44
    */
   public class LogProcessor implements Processor<byte[],byte[]> {
   	private ProcessorContext context;
   
   	@Override
   	public void init(ProcessorContext processorContext) {
   		this.context = processorContext;
   	}
   
   	@Override
   	public void process(byte[] dummy, byte[] line) {
   		String input = new String(line);
   
   		if(input.contains("PRODUCT_RATING_PREFIX:")){
   			System.out.println("book rating coming!!!!" + input);
   			input = input.split("PRODUCT_RATING_PREFIX:")[1].trim();
   			context.forward("logProcessor".getBytes(), input.getBytes());
   		}
   	}
   
   	@Override
   	public void punctuate(long l) {
   
   	}
   
   	@Override
   	public void close() {
   
   	}
   }
   ```

   

10. 编码`MyEventTimeExtractor.java`

    ```java
    package com.wan.kafkastreaming;
    
    import org.apache.kafka.clients.consumer.ConsumerRecord;
    import org.apache.kafka.streams.processor.TimestampExtractor;
    
    /**
     * @author wanfeng
     * @date 2021/3/13 18:44
     */
    public class MyEventTimeExtractor implements TimestampExtractor {
    	@Override
    	public long extract(ConsumerRecord<Object, Object> consumerRecord, long l) {
    		return 0;
    	}
    }
    ```

    