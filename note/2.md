---
title: Docker
date: 2021-03-10 19:42:05
description: 
categories: 
tags: 
	- 

---
# 基本软件安装

## 1.安装MongoDB


> 参考菜鸟教程：[Linux平台安装MongoDB](https://www.runoob.com/mongodb/mongodb-linux-install.html)
>
> MongoDB下载网址：https://www.mongodb.com/try/download/community

![image-20210310194520446](F:\BookRecSys\note\images\image-20210310194520446.png)

复制后的链接为：https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel62-4.2.13-rc2.tgz

具体操作如下：

```shell
cd /usr/local
wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel62-4.2.13-rc2.tgz
tar -zxvf mongodb-linux-x86_64-rhel62-4.2.13-rc2.tgz 
mv mongodb-linux-x86_64-rhel62-4.2.13-rc2 mongodb
cd mongodb
mkdir data
cd data
mkdir db
mkdir logs
cd logs
touch mongodb.log
cd ..
touch mongodb.conf
vim mongodb.conf
######################################
#端口号port = 27017
#数据目录
dbpath = /usr/local/mongodb/data/db
#日志目录
logpath = /usr/local/mongodb/data/logs/mongodb.log
#设置后台运行
fork = true
#日志输出方式
logappend = true
#开启认证
#auth = true
#用于远程连结
bind_ip = 0.0.0.0
#####################################
sudo /usr/local/mongodb/bin/mongod -config /usr/local/mongodb/data/mongodb.conf

/usr/local/mongodb/bin/mongo

sudo /usr/local/mongodb/bin/mongod -shutdown -config /usr/local/mongodb/data/mongodb.conf

```

![image-20210310204740834](F:\BookRecSys\note\images\image-20210310204740834.png)

## 2.卸载系统自带JDK

因为安装的CentOS 6.8自带的JDK版本为1.7，而Spark在2.2版本之后不再支持JDK1.7，所以需要重新安装JDK1.8

### 1.卸载JDK1.7

输入命令`yum list installed | grep java`就可以查看系统中的JDK，如图

![image-20210312201952060](F:\BookRecSys\note\images\image-20210312201952060.png)

卸载` yum -y remove java-1.6.0-openjdk.x86_64`,` yum -y remove java-1.7.0-openjdk.x86_64`

完成后，输入命令`java -version`会发现JDK已经完全卸载，接下来安装JDK1.8

### 2.安装JDK1.8

1.首先需要在Oracle的官网下载JDK1.8的安装包（官网下载需要注册账号），这里我本地有JDK1.8的包，直接上传到虚拟机中进行安装，上传过程略过，安装包位置在`/usr/local/java`

2.解压

`tar -zxvf jdk-8u251-linux-x64.tar.gz`

3.重命名

`mv jdk1.8.0_251/ jdk`

4.配置环境变量

`vim /etc/profile`

5.在文件末尾添加如下内容

```profile
export JAVA_HOME=/usr/local/java/jdk
export PATH=$PATH:$JAVA_HOME/bin
```

6.保存退出后，使配置生效

`source /etc/profile`

7.再次输入`java -version`会出现相应的JDK版本信息，JDK安装完成！

## 3.安装Spark

Spark的下载官网：https://spark.apache.org/downloads.html

![image-20210312193903621](F:\BookRecSys\note\images\image-20210312193903621.png)

1.选择相应的版本后点击紫色箭头会出现清华镜像的下载地址，复制链接地址在虚拟机中通过wget下载，具体操作如下：

> 注意：这里我开始选择2.4.7版本的Spark，在运行程序时会与原视频教程所给出依赖的版本不一致，为简单起见，选择与原视频教程相同的Spark版本，即Spark2.1.1
>
> 下载地址为：https://archive.apache.org/dist/spark/spark-2.1.1/spark-2.1.1-bin-hadoop2.7.tgz

```shell
# 下载Spark
wget https://archive.apache.org/dist/spark/spark-2.1.1/spark-2.1.1-bin-hadoop2.7.tgz
# 解压
tar -zxvf spark-2.1.1-bin-hadoop2.7.tgz
# 重命名
mv spark-2.1.1-bin-hadoop2.7 spark
# 进入spark目录
cd spark
# 复制slave配置文件
 cp ./conf/slaves.template ./conf/slaves
 # 编辑slaves文件，在文件末尾添加本机主机名
 vim ./conf/slaves
 -------
 localhost #原文件内容，将这个覆盖为自己的主机名
 book #主机名
 -------
# 复制Spark-Env文件
cp ./conf/spark-env.sh.template ./conf/spark-env.sh
# 编辑Spark-Env文件,添加spark master的主机名和端口
vim ./conf/spark-env.sh
-------
SPARK_MASTER_HOST=book       #添加spark master的主机名
SPARK_MASTER_PORT=7077       #添加spark master的端口号
JAVA_HOME=/usr/local/java/jdk  #这里我开始没有添加会报错，请注意
-------
```

2.上述操作完成后，启动Spark：

` ./sbin/start-all.sh`

> 启动时会让输入root账户密码

3.此时访问`http://192.168.2.88:8080`

![image-20210312204119546](F:\BookRecSys\note\images\image-20210312204119546.png)

4.Spark关闭命令

`./sbin/stop-all.sh`

5.Spark安装完成！

## 4.安装Redis

1.下载Redis安装包

`  wget http://download.redis.io/releases/redis-4.0.2.tar.gz`

2.解压

` tar -zxvf redis-4.0.2.tar.gz `

3.重命名

`mv redis-4.0.2 redis`

4.进入Redis源码目录

`cd redis`

5.安装编译软件GCC

`yum install -y gcc`

> 如果这里安装失败，出现yum的错误，可参见问题文档部分的说明

6.`make`

7.`cd src`

8.`make install`

9.将redis根目录下的`redis.conf`复制到`/etc/`下

10.`vim /etc/redis.conf`

```conf
daemonize yes
bind 0.0.0.0
```

11.运行redis，在`/redis/src`目录下

`redis-server /etc/redis.conf`

12.进入redis

`redis-cli`

13.停止redis

`redis-cli shutdown`

14.完成

## 5.安装Zookeeper

> 官网下载地址：http://archive.apache.org/dist/zookeeper/zookeeper-3.4.10/

1. 下载Zookeeper安装包

   `wget http://archive.apache.org/dist/zookeeper/zookeeper-3.4.10/zookeeper-3.4.10.tar.gz`

2. 解压

   ` tar -zxvf zookeeper-3.4.10.tar.gz `

3. 重命名

   `mv zookeeper-3.4.10 zookeeper`

4. 进入Zookeeper根目录并创建data数据目录

   `cd zookeeper`

   `mkdir data`

5. 复制Zookeeper配置文件

   ` cp ./conf/zoo_sample.cfg ./conf/zoo.cfg`

6. 修改Zookeeper配置文件

   `vim ./conf/zoo.cfg`

   ```cfg
   #将dataDIr修改为刚刚创建的data目录路径
   dataDir=/usr/local/zookeeper/data
   ```

   

7. 启动Zookeeper服务

   `./bin/zkServer.sh start`

8. 查看Zookeeper服务

   `./bin/zkServer.sh status`

9. 停止Zookeeper服务

   `./bin/zkServer.sh stop`

## 6.安装Kafka

> 官网下载地址：http://kafka.apache.org/downloads.html

1. 下载Kafka安装包

   `wget https://archive.apache.org/dist/kafka/0.10.2.1/kafka_2.12-0.10.2.1.tgz`

2. 解压

   ` tar -zxvf kafka_2.12-0.10.2.1.tgz`

3. 重命名

   `mv kafka_2.12-0.10.2.1 kafka`

4. 进入Kafka根目录并修改配置文件

   `cd kafka`

   `vim ./config/server.properties`

   ```shell
   #添加如下内容
   host.name=book
   port=9092
   zookeeper.connect=book:2181
   ```

5. 启动Kafka服务

   > 注意！启动Kafka之前需要启动Zookeeper服务

   `./bin/kafka-server-start.sh -daemon ./config/server.properties`

6. 关闭Kafka服务

   `./bin/kafka-server-stop.sh`

## 7.安装Flume

> 官网下载地址：http://archive.apache.org/dist/flume/1.8.0/

1. 下载Flume安装包

   `wget http://archive.apache.org/dist/flume/1.8.0/apache-flume-1.8.0-bin.tar.gz`

2. 解压

   ` tar -zxvf apache-flume-1.8.0-bin.tar.gz`

3. 重命名

   `mv apache-flume-1.8.0-bin flume`

4. 等待系统部署

## 8.安装Elasticsearch

>https://www.elastic.co/cn/downloads/elasticsearch#preview-release

1. 下载ES安装包（选择5.6.10版本）

   `wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.6.10.tar.gz`

2. 解压

   `tar -zxvf elasticsearch-5.6.10.tar.gz`

3. 重命名

   `mv elasticsearch-5.6.10 elasticsearch`

4. 在`elasticsearch`目录下创建两个目录

   `mkdir data `

   `mkdir logs`

5. 修改配置文件

   ```yml
   cluster.name: book-elasticsearch
   # Use a descriptive name for the node:
   node.name: node-1
   path.data: /usr/local/elasticsearch/data
   # Path to log files:
   path.logs: /usr/local/elasticsearch/logs
   #network.host: 192.168.0.1
   network.host: 0.0.0.0
   ```

   

6. 启动（非root用户）

   首先为用户增加权限

   `chown -R feng /usr/local/elasticsearch`

   启动

   `su feng`

   `./bin/elasticsearch`

   出现错误

   ```shell
   java.lang.UnsupportedOperationException: seccomp unavailable: CONFIG_SECCOMP not compiled into kernel
   ```

   解决方法

   `vim ./conf/elasticsearch.conf`

   ```conf
   # ----------------------------------- Memory -----------------------------------
   #
   # Lock the memory on startup:
   #
   #bootstrap.memory_lock: true
   # 增加一行
   bootstrap.system_call_filter: false
   ```

   

   ```shell
   ERROR: [4] bootstrap checks failed
   [1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536]
   [2]: max number of threads [1024] for user [feng] is too low, increase to at least [2048]
   [3]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]
   [4]: system call filters failed to install; check the logs and fix your configuration or disable system call filters at your own risk
   ```

   解决方法

   - `sysctl -w vm.max_map_count=262144`

   - `vim /etc/security/limits.conf`

   - ```conf
     #<domain>      <type>  <item>         <value>
     #
     *                soft    nofile          65536
     *                hard    nofile          131072
     *                soft    nproc           2048
     *                hard    nproc           4096
     ```

   - `vim /etc/security/limits.d/90-nproc.conf`

   - ```conf
     *          soft    nproc     4096
     ```
     
   - 用户重新登录才会生效 
   
   - 查看状态 http://192.168.2.88:9200/
   
   - ```
     {
       "name" : "node-1",
       "cluster_name" : "book-elasticsearch",
       "cluster_uuid" : "1EVjxJgjQ1CSHdPgg7fG3Q",
       "version" : {
         "number" : "5.6.10",
         "build_hash" : "b727a60",
         "build_date" : "2018-06-06T15:48:34.860Z",
         "build_snapshot" : false,
         "lucene_version" : "6.6.1"
       },
       "tagline" : "You Know, for Search"
     }
     ```
   
   - 