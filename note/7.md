# 构建日志数据实时信息流

## Kafka的相关设置

1. 创建两个topic

   `./bin/kafka-topics.sh --create --zookeeper book:2181 --replication-factor 1 --partitions 1 --topic recommender`

   `./bin/kafka-topics.sh --create --zookeeper book:2181 --replication-factor 1 --partitions 1 --topic log`

2. 开启producer

   `./bin/kafka-console-producer.sh --broker-list book:9092 --topic recommender`

   此时终端会保持输入状态，当输入内容后，相应topic的consumer会收到该输入内容

3. 创建consumer

   `./bin/kafka-console-consumer.sh --bootstrap-server book:9092 --topic recommender`

![image-20210313194628204](F:\BookRecSys\note\images\image-20210313194628204.png)

## Flume的相关设置

```properties
agent.sources = exectail
agent.channels = memoryChannel
agent.sinks = kafkasink

agent.sources.exectail.type = exec
agent.sources.exectail.command = tail -f /usr/local/bookrecsys/book.log
agent.sources.exectail.interceptors = i1
agent.sources.exectail.interceptors.i1.type = regex_filter
agent.sources.exectail.interceptors.i1.regex = .+MOVIE_RATING_PREFIX.+
agent.sources.exectail.channels = memoryChannel

agent.sinks.kafkasink.type = org.apache.flume.sink.kafka.KafkaSink
agent.sinks.kafkasink.kafka.topic = log
agent.sinks.kafkasink.kafka.bootstrap.servers = book:9092
agent.sinks.kafkasink.kafka.producer.acks = 1
agent.sinks.kafkasink.kafka.flumeBatchSize = 20

agent.sinks.kafkasink.channel = memoryChannel

agent.channels.memoryChannel.type = memory
agent.channels.memoryChannel.capacity = 10000
```

解释一下这里的流程

> 先启动KafkaStreaming
>
> 然后启动Flume
>
> 无法收到信息
>
> ---
>
> 先启动Flume
>
> 然后启动KafkaStreaming
>
> 此时KafkaStreaming能接收到Flume传递的信息，但是感觉消息并不及时（在启动Flume和KafkaStreaming之前，监听的日志文件中有四条有前缀的数据，启动Flume然后启动KafkaStreaming之后能收到已有的四条信息，但是再向日志中写入一条信息后，并不能接收到。。。重启Flume后能收到最新添加的信息，重启KafkaStreaming之后无法收到任何信息）

![image-20210316211051366](F:\BookRecSys\note\images\image-20210316211051366.png)

![image-20210316211122054](F:\BookRecSys\note\images\image-20210316211122054.png)